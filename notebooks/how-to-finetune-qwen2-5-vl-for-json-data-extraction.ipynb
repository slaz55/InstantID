{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqr_twmaBuh0"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Fine-tune Qwen2.5-VL on JSON Data Extraction Dataset\n",
        "\n",
        "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb)\n",
        "[![dataset](https://app.roboflow.com/images/download-dataset-badge.svg)](https://universe.roboflow.com/roboflow-jvuqo/pallet-load-manifest)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen2.5-VL)\n",
        "\n",
        "Qwen2.5-VL is the latest vision-language model in the Qwen series, delivering state-of-the-art capabilities for understanding and analyzing images, text, and documents. Available in three sizes (3B, 7B, and 72B), it excels in tasks such as precise object localization with bounding boxes, enhanced OCR for multi-language and multi-orientation text recognition, and structured data extraction from formats like invoices, forms, and tables. With advanced image recognition spanning plants, animals, landmarks, and products, Qwen2.5-VL sets a new benchmark for multimodal understanding, catering to diverse domains including finance, commerce, and digital intelligence.\n",
        "\n",
        "Designed for efficiency and accuracy, Qwen2.5-VL introduces innovations such as enhanced visual encoders with dynamic resolution ViT and Window Attention for computational efficiency. The 72B-Instruct model achieves competitive performance in benchmarks like document understanding and reasoning, outperforming models of similar sizes without task-specific fine-tuning. The smaller 3B and 7B models provide edge AI solutions while improving upon previous iterations, making Qwen2.5-VL a versatile choice for a wide range of vision-language applications.\n",
        "\n",
        "![Qwen2.5-VL](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/qwen2.5vl_arc.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJtqx0uMqaf"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krna9v-RMv-J"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune Qwen2.5-VL, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYoqF4DdW-4A"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune Qwen2.5-VL, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
      ],
      "metadata": {
        "id": "2GYHZC1vQph_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG0bAYL0iya5"
      },
      "source": [
        "### Check GPU availability\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-eyhs3RiizX",
        "outputId": "be675a43-051a-4cb6-b83c-3033a7b0f96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep 29 21:24:46 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   34C    P0             58W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp9yYB7qi60l"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I3IHLEk8i5dm"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"transformers>=4.49.0\" accelerate peft bitsandbytes \"qwen-vl-utils[decord]==0.0.8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AFSmJ0EOON3"
      },
      "source": [
        "## Download and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "08P-2ckaOSPD"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"roboflow>=1.1.54\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import download_dataset\n",
        "\n",
        "dataset = download_dataset(\"https://app.roboflow.com/roboflow-jvuqo/pallet-load-manifest-json/2\", \"jsonl\")"
      ],
      "metadata": {
        "id": "JfduTFWURE0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQBwo_u5OkTh"
      },
      "outputs": [],
      "source": [
        "!head -n 5 {dataset.location}/train/annotations.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0qs8KEiYR0K"
      },
      "outputs": [],
      "source": [
        "!sed -i 's/<JSON>/extract data in JSON format/g' {dataset.location}/train/annotations.jsonl\n",
        "!sed -i 's/<JSON>/extract data in JSON format/g' {dataset.location}/valid/annotations.jsonl\n",
        "!sed -i 's/<JSON>/extract data in JSON format/g' {dataset.location}/test/annotations.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN0poRhHY0wj"
      },
      "outputs": [],
      "source": [
        "!head -n 5 {dataset.location}/train/annotations.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95iVR1BsPLHT"
      },
      "source": [
        "## Data formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nZRjRSRQg2H"
      },
      "outputs": [],
      "source": [
        "SYSTEM_MESSAGE = \"\"\"You are a Vision Language Model specialized in extracting structured data from visual representations of palette manifests.\n",
        "Your task is to analyze the provided image of a palette manifest and extract the relevant information into a well-structured JSON format.\n",
        "The palette manifest includes details such as item names, quantities, dimensions, weights, and other attributes.\n",
        "Focus on identifying key data fields and ensuring the output adheres to the requested JSON structure.\n",
        "Provide only the JSON output based on the extracted information. Avoid additional explanations or comments.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgJZYkNqQtEA"
      },
      "outputs": [],
      "source": [
        "def format_data(image_directory_path, entry):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_MESSAGE}],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": image_directory_path + \"/\" + entry[\"image\"],\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": entry[\"prefix\"],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": entry[\"suffix\"]}],\n",
        "        },\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0imUaavvPNLn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class JSONLDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.jsonl_file_path = jsonl_file_path\n",
        "        self.image_directory_path = image_directory_path\n",
        "        self.entries = self._load_entries()\n",
        "\n",
        "    def _load_entries(self):\n",
        "        entries = []\n",
        "        with open(self.jsonl_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data = json.loads(line)\n",
        "                entries.append(data)\n",
        "        return entries\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if idx < 0 or idx >= len(self.entries):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
        "        image = Image.open(image_path)\n",
        "        return image, entry, format_data(self.image_directory_path, entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRyARSnbPif6"
      },
      "outputs": [],
      "source": [
        "train_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/train/annotations.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/train\",\n",
        ")\n",
        "valid_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/valid/annotations.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/valid\",\n",
        ")\n",
        "test_dataset = JSONLDataset(\n",
        "    jsonl_file_path=f\"{dataset.location}/test/annotations.jsonl\",\n",
        "    image_directory_path=f\"{dataset.location}/test\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCUUJDwESjvT"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOw7oZzmSov3"
      },
      "source": [
        "## Model loading and configuration\n",
        "\n",
        "In this section, we load the **Qwen2.5-VL** model with optional **LoRA** (Low-Rank Adaptation) or **QLoRA** (Quantized LoRA). LoRA injects small trainable weights (the “low-rank matrices”) into the model’s layers, saving significant memory and compute compared to fine-tuning the entire model. QLoRA further applies **4-bit quantization** to reduce memory usage while still preserving much of the model’s performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwbHICvXtE2g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
        "\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_QLORA = True\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "if USE_QLORA:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_type=torch.bfloat16\n",
        "    )\n",
        "\n",
        "\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config if USE_QLORA else None,\n",
        "    torch_dtype=torch.bfloat16)\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM5d_ITF4Dwl"
      },
      "source": [
        "The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBGIf18iS7hq"
      },
      "outputs": [],
      "source": [
        "MIN_PIXELS = 256 * 28 * 28\n",
        "MAX_PIXELS = 1280 * 28 * 28\n",
        "\n",
        "processor = Qwen2_5_VLProcessor.from_pretrained(MODEL_ID, min_pixels=MIN_PIXELS, max_pixels=MAX_PIXELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX0X8YIdAfvc"
      },
      "source": [
        "## Data collation and tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6eUM3vxq_jA"
      },
      "source": [
        "### Train Collate Function\n",
        "We use a masking technique that replaces padding tokens, special image tokens, and tokens from system and user turns with -100 so that the loss function ignores them and only evaluates the assistant's response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0TQqWjFR6D4"
      },
      "outputs": [],
      "source": [
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "\n",
        "def train_collate_fn(batch):\n",
        "    _, _, examples = zip(*batch)\n",
        "\n",
        "    texts = [\n",
        "        processor.apply_chat_template(example, tokenize=False)\n",
        "        for example\n",
        "        in examples\n",
        "    ]\n",
        "    image_inputs = [\n",
        "        process_vision_info(example)[0]\n",
        "        for example\n",
        "        in examples\n",
        "    ]\n",
        "\n",
        "    model_inputs = processor(\n",
        "        text=texts,\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    labels = model_inputs[\"input_ids\"].clone()\n",
        "\n",
        "    # mask system message and image token IDs in the labels\n",
        "    for i, example in enumerate(examples):\n",
        "        sysuser_conv = example[:-1]\n",
        "        sysuser_text = processor.apply_chat_template(sysuser_conv, tokenize=False)\n",
        "        sysuser_img, _ = process_vision_info(sysuser_conv)\n",
        "\n",
        "        sysuser_inputs = processor(\n",
        "            text=[sysuser_text],\n",
        "            images=[sysuser_img],\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "\n",
        "        sysuser_len = sysuser_inputs[\"input_ids\"].shape[1]\n",
        "        labels[i, :sysuser_len] = -100\n",
        "\n",
        "    input_ids = model_inputs[\"input_ids\"]\n",
        "    attention_mask = model_inputs[\"attention_mask\"]\n",
        "    pixel_values = model_inputs[\"pixel_values\"]\n",
        "    image_grid_thw = model_inputs[\"image_grid_thw\"]\n",
        "\n",
        "    return input_ids, attention_mask, pixel_values, image_grid_thw, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOZpT-7rE6I"
      },
      "source": [
        "### Evaluation Collate Function\n",
        "\n",
        "In the evaluation function, we separate out the suffixes (i.e., ground-truth target text) for later comparison. The line `examples = [e[:2] for e in examples]` effectively drops the assistant’s final turn from the template so that we only feed the model system + user and then generate the assistant output ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wjSkkdFrEJ2"
      },
      "outputs": [],
      "source": [
        "def evaluation_collate_fn(batch):\n",
        "    _, data, examples = zip(*batch)\n",
        "    suffixes = [d[\"suffix\"] for d in data]\n",
        "\n",
        "    # drop the assistant portion so the model must generate it\n",
        "    examples = [e[:2] for e in examples]\n",
        "\n",
        "    texts = [\n",
        "        processor.apply_chat_template(example, tokenize=False)\n",
        "        for example\n",
        "        in examples\n",
        "    ]\n",
        "    image_inputs = [\n",
        "        process_vision_info(example)[0]\n",
        "        for example\n",
        "        in examples\n",
        "    ]\n",
        "\n",
        "    model_inputs = processor(\n",
        "        text=texts,\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    input_ids = model_inputs[\"input_ids\"]\n",
        "    attention_mask = model_inputs[\"attention_mask\"]\n",
        "    pixel_values = model_inputs[\"pixel_values\"]\n",
        "    image_grid_thw = model_inputs[\"image_grid_thw\"]\n",
        "\n",
        "    return input_ids, attention_mask, pixel_values, image_grid_thw, suffixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mllDEl689DB0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=train_collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=evaluation_collate_fn, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=evaluation_collate_fn, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiE5ZD0d9Tuc"
      },
      "outputs": [],
      "source": [
        "input_ids, attention_mask, pixel_values, image_grid_thw, labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqqSj-jP9p7j"
      },
      "outputs": [],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0RhhQr8_dKK"
      },
      "outputs": [],
      "source": [
        "processor.batch_decode(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZHzY5Ip_57i"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(profile=\"full\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj36ezPd_8Yr"
      },
      "outputs": [],
      "source": [
        "attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdh9x_gd_9xS"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snlIGXRxAJJY"
      },
      "outputs": [],
      "source": [
        "for input_id, label in zip(input_ids[0], labels[0]):\n",
        "    print(processor.decode(input_id), label.item() if label.item() == -100 else processor.decode(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYJjer_OXdjQ"
      },
      "outputs": [],
      "source": [
        "input_ids, attention_mask, pixel_values, image_grid_thw, suffixes = next(iter(valid_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ze60L0XpvD"
      },
      "outputs": [],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9re9B46XoVN"
      },
      "outputs": [],
      "source": [
        "processor.batch_decode(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulf2pFDQAlck"
      },
      "source": [
        "## Training with PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KJlZ71cAncz"
      },
      "outputs": [],
      "source": [
        "!pip install -q lightning nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe6BP0pXrkfK"
      },
      "source": [
        "The `edit_distance` (from `nltk`) measures how many character-level edits (insertion, deletion, substitution) are needed to transform the predicted text into the ground truth. We normalize it by the length of the longer string to get a score between 0 (identical) and 1 (completely different)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrGQK76AAwbP"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "from nltk import edit_distance\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "class Qwen2_5_Trainer(L.LightningModule):\n",
        "    def __init__(self, config, processor, model):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids, attention_mask, pixel_values, image_grid_thw, labels = batch\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values,\n",
        "            image_grid_thw=image_grid_thw,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
        "        input_ids, attention_mask, pixel_values, image_grid_thw, suffixes = batch\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            pixel_values=pixel_values,\n",
        "            image_grid_thw=image_grid_thw,\n",
        "            max_new_tokens=1024\n",
        "        )\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids) :]\n",
        "            for in_ids, out_ids\n",
        "            in zip(input_ids, generated_ids)]\n",
        "\n",
        "        generated_suffixes = processor.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "        scores = []\n",
        "        for generated_suffix, suffix in zip(generated_suffixes, suffixes):\n",
        "            score = edit_distance(generated_suffix, suffix)\n",
        "            score = score / max(len(generated_suffix), len(suffix))\n",
        "            scores.append(score)\n",
        "\n",
        "            print(\"generated_suffix\", generated_suffix)\n",
        "            print(\"suffix\", suffix)\n",
        "            print(\"score\", score)\n",
        "\n",
        "        score = sum(scores) / len(scores)\n",
        "        self.log(\"val_edit_distance\", score, prog_bar=True, logger=True, batch_size=self.config.get(\"batch_size\"))\n",
        "        return scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.config.get(\"lr\"))\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config.get(\"batch_size\"),\n",
        "            collate_fn=train_collate_fn,\n",
        "            shuffle=True,\n",
        "            num_workers=10,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            valid_dataset,\n",
        "            batch_size=self.config.get(\"batch_size\"),\n",
        "            collate_fn=evaluation_collate_fn,\n",
        "            num_workers=10,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F16kwt2C0TU"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"max_epochs\": 10,\n",
        "    \"batch_size\": 4,\n",
        "    \"lr\": 2e-4,\n",
        "    \"check_val_every_n_epoch\": 2,\n",
        "    \"gradient_clip_val\": 1.0,\n",
        "    \"accumulate_grad_batches\": 8,\n",
        "    \"num_nodes\": 1,\n",
        "    \"warmup_steps\": 50,\n",
        "    \"result_path\": \"qwen2.5-3b-instruct-palette-manifest\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBubewIvDQQr"
      },
      "outputs": [],
      "source": [
        "model_module = Qwen2_5_Trainer(config, processor, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxVpNY30DXTk"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import Callback\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")\n",
        "\n",
        "\n",
        "class SaveCheckpoint(Callback):\n",
        "    def __init__(self, result_path):\n",
        "        self.result_path = result_path\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        checkpoint_path = f\"{self.result_path}/{self.epoch}\"\n",
        "        os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "        pl_module.processor.save_pretrained(checkpoint_path)\n",
        "        pl_module.model.save_pretrained(checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "        self.epoch += 1\n",
        "\n",
        "    def on_train_end(self, trainer, pl_module):\n",
        "        checkpoint_path = f\"{self.result_path}/latest\"\n",
        "        os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "        pl_module.processor.save_pretrained(checkpoint_path)\n",
        "        pl_module.model.save_pretrained(checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk7WuMqpDaTQ"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[0],\n",
        "    max_epochs=config.get(\"max_epochs\"),\n",
        "    accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
        "    check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
        "    gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
        "    limit_val_batches=1,\n",
        "    num_sanity_val_steps=0,\n",
        "    log_every_n_steps=10,\n",
        "    callbacks=[SaveCheckpoint(result_path=config[\"result_path\"]), early_stopping_callback],\n",
        ")\n",
        "\n",
        "trainer.fit(model_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN6umVTTSQQg"
      },
      "source": [
        "### Run inference with fine-tuned Qwen2.5-VL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBKk5cP7uAsG"
      },
      "outputs": [],
      "source": [
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"/content/qwen2.5-3b-instruct-palette-manifest/latest\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "processor = Qwen2_5_VLProcessor.from_pretrained(\n",
        "    \"/content/qwen2.5-3b-instruct-palette-manifest/latest\",\n",
        "    min_pixels=MIN_PIXELS,\n",
        "    max_pixels=MAX_PIXELS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CHUNdDevcgp"
      },
      "outputs": [],
      "source": [
        "def run_inference(model, processor, conversation, max_new_tokens=1024, device=\"cuda\"):\n",
        "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, _ = process_vision_info(conversation)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids\n",
        "        in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return output_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA0N9Jvx_2CN"
      },
      "outputs": [],
      "source": [
        "image, entry, conversation = test_dataset[0]\n",
        "conversation = conversation[:2]\n",
        "suffix = entry[\"suffix\"]\n",
        "suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkfCTHqrvoRs"
      },
      "outputs": [],
      "source": [
        "generated_suffix = run_inference(model, processor, conversation)\n",
        "generated_suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_gwjEeff_Nw"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7YJCm5ngJKu"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def side_by_side_diff_divs(text1, text2):\n",
        "    lines1 = text1.splitlines()\n",
        "    lines2 = text2.splitlines()\n",
        "\n",
        "    original_output = []\n",
        "    modified_output = []\n",
        "\n",
        "    for line1, line2 in zip(lines1, lines2):\n",
        "        words1 = line1.split()\n",
        "        words2 = line2.split()\n",
        "\n",
        "        matcher = SequenceMatcher(None, words1, words2)\n",
        "\n",
        "        original_line = []\n",
        "        modified_line = []\n",
        "\n",
        "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
        "            if tag == 'replace':\n",
        "                original_line.append(f\"<span class='diff-remove'>{' '.join(words1[i1:i2])}</span>\")\n",
        "                modified_line.append(f\"<span class='diff-add'>{' '.join(words2[j1:j2])}</span>\")\n",
        "            elif tag == 'delete':\n",
        "                original_line.append(f\"<span class='diff-remove'>{' '.join(words1[i1:i2])}</span>\")\n",
        "            elif tag == 'insert':\n",
        "                modified_line.append(f\"<span class='diff-add'>{' '.join(words2[j1:j2])}</span>\")\n",
        "            elif tag == 'equal':\n",
        "                original_line.append(' '.join(words1[i1:i2]))\n",
        "                modified_line.append(' '.join(words2[j1:j2]))\n",
        "\n",
        "        original_output.append(' '.join(original_line) + \"<br>\")\n",
        "        modified_output.append(' '.join(modified_line) + \"<br>\")\n",
        "\n",
        "    original_html = \"<br>\" + ''.join(original_output) + \"<br>\"\n",
        "    modified_html = \"<br>\" + ''.join(modified_output) + \"<br>\"\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 0; padding: 0; }}\n",
        "            .container {{ display: flex; align-items: flex-start; }}\n",
        "            .column {{\n",
        "                flex: 1;\n",
        "                padding: 10px;\n",
        "                white-space: pre-wrap;\n",
        "                text-align: left;\n",
        "            }}\n",
        "            .diff-remove {{\n",
        "                background-color: #d9534f;  /* Dark red */\n",
        "                color: white;\n",
        "                text-decoration: line-through;\n",
        "                border-radius: 4px;\n",
        "                padding: 2px 4px;\n",
        "            }}\n",
        "            .diff-add {{\n",
        "                background-color: #5cb85c;  /* Dark green */\n",
        "                color: white;\n",
        "                border-radius: 4px;\n",
        "                padding: 2px 4px;\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"container\">\n",
        "            <div class=\"column\" style=\"border-right: 1px solid #ccc;\">\n",
        "                {original_html}\n",
        "            </div>\n",
        "            <div class=\"column\">\n",
        "                {modified_html}\n",
        "            </div>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JiPaNZvoHSJ"
      },
      "outputs": [],
      "source": [
        "html_diff = side_by_side_diff_divs(suffix, generated_suffix)\n",
        "display(HTML(html_diff))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}